---
title: Running Gaussian 03 on Legion
layout: docs
---
[[Category:Bash script pages]]
[[Category:Legion]]
Gaussian 03 (G03) revision E.01 is available on legion and currently can be used in serial mode and shared memory parallel mode within single nodes using at most four processors. Access to G03 is enabled by a module file and being a member of the appropriate reserved application group. Please email [mailto:rc-support@ucl.ac.uk rc-support@ucl.ac.uk] to get your userid added to the G03 group.

All G03 jobs apart from small test jobs (4 cores and less than 5 minutes runtime) must be submitted as batch jobs. Before you can run G03 interactively you need to load the G03 module and run an initialisation script using:
<code>
 module load gaussian/g03_e01/pgi
 . $g03root/g03/bsd/g03.profile
</code>
You can use:
<code>
 module list
</code>
to check that the module is loaded. Output should look similar to this:
<code>
 Currently Loaded Modulefiles:
  1) ucl                          6) nedit/5.6
  2) compilers/intel/11.1/072     7) mrxvt/0.5.4
  3) mkl/10.2.5/035               8) rcops/1.0
  4) mpi/qlogic/1.2.7/intel       9) gaussian/g03_e01/pgi
  5) sge/6.2u3
</code>
You should now be able to run G03 using:
<code>
g03 < myG03input > myG03output
</code>
for example.

To submit batch jobs to the cluster you will need a runscript. Here is a simple example G03 runscript for shared memory jobs including comments:
{|style="background-color:#F9F9F9;" width=100%
|-
|
<code>
 #!/bin/bash -l

 # Batch script to run a Gaussian 03 job on Legion with the upgraded
 # software stack under SGE.
 #
 # 21st Sept 2010
 #
 # Based on openmp.sh by:
 #
 # Owain Kenway, Research Computing, 16/Sept/2010

 #$ -S /bin/bash

 # 1. Request 12 hours of wallclock time (format hours:minutes:seconds).
 #$ -l h_rt=12:0:0

 # 2. Request 4 gigabyte of RAM.
 #$ -l mem=4G

 # 3. Set the name of the job.
 #$ -N G03_job1

 # 4. Select  4 OpenMP threads (the most possible on Legion).
 #$ -l thr=4

 # 5. Select the project that this job will run under.
 # Find <your_project_id> by running the command "groups"
 #$ -P <your_project_name>

 # 6. Set the working directory to somewhere in your scratch space.  This is
 # a necessary step with the upgraded software stack as compute nodes cannot
 # write to $HOME.
 # Replace "<your_UCL_id>" with your UCL user ID :)
 #$ -wd /home/<your_UCL_id>/Scratch/G03_output

 #Old PBS setting: -N G03_job1
 #Old PBS setting: -l nodes=1:ppn=4,naccesspolicy=singlejob,qos=parallel
 #Old PBS setting: -l pvmem=8g,walltime=12:00:00
 #Old PBS setting: -A ucl/<consortium name>/<project name>

 # Setup G03 runtime environment

 module load gaussian/g03_e01/pgi
 mkdir -p $GAUSS_SCRDIR
 . $g03root/g03/bsd/g03.profile

 echo "GAUSS_SCRDIR = $GAUSS_SCRDIR"
 echo ""
 echo "Running: g03 < $g03infile > $g03outfile"
 g03 < $g03infile > $g03outfile
</code>
|}

This is available on Legion in:
<code>
 /shared/ucl/apps/Gaussian/G03_E01/run-g03.sh
</code>
Please copy if you wish and edit it to suit your jobs. You will need to change the ''-P <your_project_name>''  and ''-wd /home/<your_UCL_id>/Scratch/G03_output'' SGE directives and may need to change the memory, wallclock time, number of threads and job name directives as well. A suitable qsub command to submit a G03 job using this runscript would be:
<code>
 qsub -v g03infile=`pwd`/MyData.com,g03outfile=MyOutput.out run-g03.sh
</code>
where ''Mydata.com'' is the file containing your G03 commands and ''MyOuput.out'' is the output file. In this example input, and your runscript files are in your current working directory. The output file is saved in the directory specified by the -wd SGE directive.

===Submitting Long Gaussian Jobs===

It is possible to obtain permission to submit single node Gaussian jobs with wallclock times between 2 and 7 days. For details of how to gain access to the 7-day Gaussian queue see [[Legion Resource Allocation#Requests for Additional Resources or Resource Reservations | Requests for Additional Resources]].

As the 7-day queue is restricted to shared memory Gaussian jobs you will need to make some changes to your runscripts:
# Include the grid engine directive:<br /><code>#$ -ac app=g03</code><br />for Gaussian 03 jobs. If the directive is not present, normal job wallclock limits apply.
# The way Gaussian is launched needs to be modified as the new queues launch g03 via a new wrapper command. The new wrapper is G03  - note the capital G! It takes arguments for Gaussian command (standard input) and output (standard output) files so need to be used like so:<br /><code>G03 commands.in output.out</code><br />where commands.in is the file containing your Gaussian commands and output.out is the file where standard output will appear. The G09 wrapper is used in the same way.

Here is a simple Gaussian 03 runscript using the new '7-day' queue:
{|style="background-color:#F9F9F9;" width=100%
|-
|
<code>
 #!/bin/bash -l

 # Batch script to run a Gaussian 03 job on Legion using 

 # the restricted '7-day' queue under SGE 

 #

 # Aug 2012

 #

 # Based on openmp.sh by:

 #

 # Owain Kenway, Research Computing, 16/Sept/2010

 #$ -S /bin/bash

 # 1. Request 12 hours of wallclock time (format hours:minutes:seconds).

 #$ -l h_rt=3:0:0

 # 2. Request 4 gigabyte of RAM.

 #$ -l mem=8G

 #$ -ac app=g03

 # 3. Set the name of the job.

 #$ -N G03_jobR

 # 4. Select  12 OpenMP threads (the most possible on Legion).

 #$ -l thr=12

 # 5. Select the project that this job will run under.

 #$ -P <your_project_name>

 # 6. Set the working directory to somewhere in your scratch space.  This is

 # a necessary step with the upgraded software stack as compute nodes cannot

 # write to $HOME.

 #

 # Note: this directory MUST exist before your job starts!

 #$ -wd /home/<your_UCL_id>/Scratch/G03_output

 # Run g03 job

 echo "GAUSS_SCRDIR = $GAUSS_SCRDIR"

 echo ""

 echo "Running: G03 $g03infile $g03outfile"

 time G03 $g03infile $g03outfile
</code>
|}

This is available on Legion in:
<code>
 /shared/ucl/apps/Gaussian/G03_E01/run-g03-res.sh
</code>
Please copy if you wish and edit it to suit your jobs. You will need to change the ''-P <your_project_name>'' and ''-wd /home/<your_UCL_id>/Scratch/G03_output'' SGE directives and may need to change the memory, wallclock time, number of threads and job name directives as well. A suitable qsub command to submit a G03 job using this runscript would be:
<code>
 qsub -v g03infile=`pwd`/MyData.com,g03outfile=MyOutput.out run-g03-res.sh
</code>
where ''Mydata.com'' is the file containing your G03 commands and ''MyOuput.out'' is the output file. In this example input, and your runscript files are in your current working directory. The output file is saved in the directory specified by the -wd SGE directive.