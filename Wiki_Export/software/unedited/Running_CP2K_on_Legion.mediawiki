---
title: Running CP2K on Legion
categories:
 - Bash script pages
 - Legion

layout: docs
---
[[Category:Bash script pages]]
[[Category:Legion]]
[[Category:Legion Applications]]
[[Category:Legion Software]]

=2.2.426=

The module for this version is contained in the chemistry modules, so you will need to load the module set that contains it before it is visible in the modules list:
<code>
 module load chemistry-modules
</code>
CP2K should then be visible as:
<code>
 module avail

 [.....]

 ----- /shared/ucl/depts/chemistry/modulefiles -----
 cp2k/2.2.426/openmpi/gnu.4.6.3/cp2k
</code>

===Submitter===

An automatic submitter is available for this version, available by loading the submission scripts module from the chemistry set:
<code>
 module load chemistry-modules
 module load submission-scripts
</code>
The alias "submitters" will then list the submitters available.

The "cp2k.submit" submitter takes up to 5 arguments, and any omitted will be asked for interactively:
<code>
 cp2k.submit «input_file» «cores» «maximum_run_time» «memory_per_core» «job_name»
</code>
So, for example:
<code>
 cp2k.submit water.inp 8 2:00:00 4G mywatermolecule
</code>
would request a job running CP2K with the input file "water.inp", on 8 cores, with a maximum runtime of 2 hours, with 4 gigabytes of memory per core, and a job name of "mywatermolecule".

===Job script===

As with the previous versions, some extra arguments to the OpenMPI mpirun command are necessary. An example job script is below:

{|style="background-color:#F9F9F9;" width=100%
|-
|
<code>
 #!/bin/bash -l
 # setting up the environment for SGE:
 #$ -S /bin/bash -l
 #$ -cwd
 #$ -N CP2K-Job

 # Set your project name here:
 #$ -P  Project_Name 

 # If necessary, alter the maximum quantity of memory here:
 #$ -l mem=2G

 # Alter the maximum run time here (hours:minutes:seconds)
 #$ -l h_rt=10:00:00

 # Alter the number of processors here:
 #$ -pe openmpi 1 

 # loading the correct modules
 module add chemistry-modules
 module add cp2k/2.2.426/openmpi/gnu.4.6.3/cp2k 

 #Modify this to name your input file
 InputFile=ACP2KFile.inp

 OutputFile=${InputFile%\.inp}.log

 mpirun --mca btl ^tcp -n $NSLOTS cp2k.popt $InputFile > $OutputFile
</code>
|}

=branch 2_1 and trunk=

To use these versions of CP2K on Legion, you will have to make considerable changes to your user environment, including using a test build of OpenMPI specifically designed for this purpose.

You also need to pick which CP2K package you wish to use: trunk, or branch 2_1 - the jobscript below selects branch 2_1 - if you require trunk, you will need to change the module specification.

===Job script===

In order to use this special version of OpenMPI, you need to make a few changes to the normal OpenMPI job script. An example job script is shown below:

{|style="background-color:#F9F9F9;" width=100%
|-
|
<code>
 #!/bin/bash -l

 # 1. Force bash as the executing shell.
 #$ -S /bin/bash

 # 2. Request ten minutes of wallclock time (format hours:minutes:seconds).
 #$ -l h_rt=0:30:0

 # 3. Request 1 gigabyte of RAM.
 #$ -l mem=1G

 # 4. Set the name of the job.
 #$ -N cp2k-branch-openmpi-gcc440-test-dev

 # 5. Select the OpenMPI parallel environment and 8 processors.
 #$ -pe openmpi  8

 # 6. Select the project that this job will run under.
 #$ -P <your_project_id>

 # 7. Set the working directory to somewhere in your scratch space.  This is
 # a necessary step with the upgraded software stack as compute nodes cannot
 # write to $HOME.
 #$ -wd /home/<your_UCL_id>/scratch/

 # 8. Run our MPI job.  

 module add compilers/gnu/4.4.0
 module add mpi/openmpi/1.4.1/gnu.4.4.0
 module add fftw/2.1.5/double/gnu.4.4.0
 module add atlas/3.8.3/gnu.4.4.0
 module add blacs/patch03/gnu.4.4.0
 module add scalapack/1.8.0/gnu.4.4.0
 module add cp2k/2_1-branch/openmpi/gnu.4.4.0

 # Delete OpenMPI PE SSH wrapper
 rm $TMPDIR/ssh

 # Need to add in --prefix $MPI_HOME as not using system OpenMPI
 mpirun --prefix $MPI_HOME -machinefile $TMPDIR/machines -np $NSLOTS `which cp2k.popt` C.inp
</code>
|}

The main differences between this script and the normal OpenMPI scripts is that we have to delete an SSH wrapper from $TMPDIR/ssh and insert --prefix $MPI_HOME into our mpirun command.  Note that we've also forced the loading of the correct modules in the job script. This assumes you have an input file called C.inp in your working directory. Amend this script as necessary.

If you put the modules into your job script as above, you should not need to have them in your .bashrc, but it's probably worth doing so just to be on the safe side, unless doing so causes clashes with other programs you want to use.